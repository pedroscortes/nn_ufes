{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a622311",
   "metadata": {},
   "source": [
    "\n",
    "# Trabalho 1: Diferenciação Automática com Grafos Computacionais\n",
    "\n",
    "## Informações Gerais\n",
    "\n",
    "- Data de Entrega: 29/06/2025\n",
    "- Pontuação: 10 pontos (+4 pontos extras)\n",
    "- O trabalho deve ser feito individualmente.\n",
    "- A entrega do trabalho deve ser realizada via sistema testr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81844651",
   "metadata": {},
   "source": [
    "## Especificação\n",
    "\n",
    "⚠️ *Esta explicação assume que você leu e entendeu os slides sobre grafos computacionais.*\n",
    "\n",
    "O trabalho consiste em implementar um sistema de diferenciação automática usando grafos computacionais e utilizar este sistema para resolver um conjunto de problemas.\n",
    "\n",
    "Para isto, devem ser definidos um tipo Tensor para representar dados (similares aos arrays do numpy) e operações (e.g., soma, subtração, etc.) que geram tensores como saída. \n",
    "\n",
    "Sempre que uma operação é realizada, é armazenado no tensor de saída referências para os seus pais, isto é, os valores usados como entrada para a operação. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacc1a1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19261d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Any, List, Tuple\n",
    "from collections.abc import Iterable\n",
    "from abc import ABC, abstractmethod\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284f531",
   "metadata": {},
   "source": [
    "### Classe NameManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd00b34",
   "metadata": {},
   "source": [
    "A classe NameManager provê uma forma conveniente de dar nomes intuitivos para tensores que resultam de operações. A idéia é tornar mais fácil para o usuário das demais classes qual operação gerou qual tensor. Ela provê os seguintes métodos públicos: \n",
    "\n",
    "- reset(): reinicia o sistema de gestão de nomes.\n",
    "- new(<basename>: str): retorna um nome único a partir do nome de base passado como argumento. \n",
    "  \n",
    "Como indicado no exemplo abaixo da classe, a idéia geral é que uma sequência de operações é feita, os nomes dos tensores sejam os nomes das operações seguidos de um número. Se forem feitas 3 operações de soma e uma de multiplicação, seus tensores de saída terão os nomes \"add:0\", \"add:1\", \"add:2\" e \"prod:0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162272a0",
   "metadata": {
    "tags": [
     "name_manager"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add:0\n",
      "in:0\n",
      "add:1\n",
      "add:2\n",
      "in:1\n",
      "prod:0\n"
     ]
    }
   ],
   "source": [
    "class NameManager:\n",
    "    \"\"\"Manages unique names for tensors created by operations\"\"\"\n",
    "    _counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def reset():\n",
    "        NameManager._counts = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _count(name):\n",
    "        if name not in NameManager._counts:\n",
    "            NameManager._counts[name] = 0\n",
    "        return NameManager._counts[name]\n",
    "\n",
    "    @staticmethod\n",
    "    def _inc_count(name):\n",
    "        assert name in NameManager._counts, f'Name {name} is not registered.'\n",
    "        NameManager._counts[name] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def new(name: str) -> str:\n",
    "        count = NameManager._count(name)\n",
    "        tensor_name = f\"{name}:{count}\"\n",
    "        NameManager._inc_count(name)\n",
    "        return tensor_name\n",
    "\n",
    "# exemplo de uso\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('add'))\n",
    "print(NameManager.new('in'))\n",
    "print(NameManager.new('prod'))\n",
    "\n",
    "NameManager.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69485a9",
   "metadata": {},
   "source": [
    "### Classe Tensor\n",
    "\n",
    "Deve ser criada uma classe `Tensor` representando um array multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448496d7",
   "metadata": {
    "tags": [
     "tensor"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union, Any, List, Tuple\n",
    "from collections.abc import Iterable\n",
    "from abc import ABC, abstractmethod\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"Multidimensional array with automatic differentiation support\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 arr: Union[np.ndarray, list, numbers.Number, 'Tensor'],\n",
    "                 parents: List['Tensor'] = None,\n",
    "                 requires_grad: bool = True,\n",
    "                 name: str = '',\n",
    "                 operation: Optional['Op'] = None,\n",
    "                 nd_support: bool = False):\n",
    "        \n",
    "        self._nd_support = nd_support\n",
    "        self._parents = parents if parents is not None else []\n",
    "        self._operation = operation\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        if isinstance(arr, Tensor):\n",
    "            self._arr = arr._arr.copy()\n",
    "            if not arr.requires_grad:\n",
    "                self.requires_grad = False\n",
    "        else:\n",
    "            try:\n",
    "                self._arr = np.array(arr, dtype=np.float64)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Cannot convert input to numpy array: {e}\")\n",
    "        \n",
    "        if not nd_support:\n",
    "            if self._arr.ndim > 2:\n",
    "                raise ValueError(f\"Tensor must be 0D, 1D, or 2D, got {self._arr.ndim}D\")\n",
    "            elif self._arr.ndim == 0:\n",
    "                self._arr = self._arr.reshape(1, 1)\n",
    "            elif self._arr.ndim == 1:\n",
    "                self._arr = self._arr.reshape(-1, 1)\n",
    "        \n",
    "        if name:\n",
    "            self._name = name\n",
    "        elif not self._parents:  \n",
    "            self._name = NameManager.new('in')\n",
    "        else:\n",
    "            self._name = 'unnamed'\n",
    "        \n",
    "        self.grad: Optional['Tensor'] = None\n",
    "        self._second_order_grad: Optional['Tensor'] = None\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reset gradients for this tensor\"\"\"\n",
    "        self.grad = None\n",
    "        self._second_order_grad = None\n",
    "    \n",
    "    def numpy(self) -> np.ndarray:\n",
    "        \"\"\"Return internal array as numpy array\"\"\"\n",
    "        return self._arr.copy()\n",
    "    \n",
    "    def item(self) -> float:\n",
    "        \"\"\"Return scalar value if tensor contains single element\"\"\"\n",
    "        if self._arr.size != 1:\n",
    "            raise ValueError(\"item() can only be called on scalar tensors\")\n",
    "        return float(self._arr.item())\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        if self._nd_support:\n",
    "            return f\"Tensor({self._arr}, name={self._name}, shape={self.shape})\"\n",
    "        else:\n",
    "            if self._arr.size > 1 and self._arr.shape != (1, 1):\n",
    "                display_arr = self._arr.squeeze()\n",
    "            else:\n",
    "                display_arr = self._arr\n",
    "            return f\"Tensor({display_arr}, name={self._name}, shape={self.shape})\"\n",
    "    \n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        return self._arr.shape\n",
    "    \n",
    "    @property\n",
    "    def T(self) -> 'Tensor':\n",
    "        \"\"\"Transpose tensor\"\"\"\n",
    "        result = Tensor(np.transpose(self._arr), requires_grad=self.requires_grad, nd_support=self._nd_support)\n",
    "        result._name = f\"{self._name}.T\"\n",
    "        return result\n",
    "    \n",
    "    def detach(self) -> 'Tensor':\n",
    "        \"\"\"Return tensor without gradient computation\"\"\"\n",
    "        return Tensor(self._arr, requires_grad=False, name=f\"{self._name}.detached\", nd_support=self._nd_support)\n",
    "    \n",
    "    def backward(self, grad: Optional['Tensor'] = None, create_graph: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Compute gradients using backpropagation with optional higher-order support\n",
    "        \n",
    "        Args:\n",
    "            grad: Gradient to backpropagate (defaults to ones)\n",
    "            create_graph: If True, creates computation graph for gradient (enables higher-order)\n",
    "        \"\"\"\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        \n",
    "        if grad is None:\n",
    "            grad = Tensor(np.ones_like(self._arr, dtype=np.float64), \n",
    "                         requires_grad=create_graph, nd_support=self._nd_support)\n",
    "            grad._name = \"ones\"\n",
    "        else:\n",
    "            grad = ensure_tensor(grad, nd_support=self._nd_support)\n",
    "            if create_graph:\n",
    "                grad.requires_grad = True\n",
    "        \n",
    "        if grad.shape != self.shape:\n",
    "            grad_arr = grad.numpy()\n",
    "            target_shape = self.shape\n",
    "            \n",
    "            if grad_arr.size >= np.prod(target_shape):\n",
    "                if grad_arr.size == np.prod(target_shape):\n",
    "                    grad_arr = grad_arr.reshape(target_shape)\n",
    "                else:\n",
    "                    while grad_arr.size > np.prod(target_shape) and grad_arr.ndim > 0:\n",
    "                        largest_dim = np.argmax(grad_arr.shape)\n",
    "                        grad_arr = np.sum(grad_arr, axis=largest_dim, keepdims=False)\n",
    "                    \n",
    "                    if grad_arr.size == np.prod(target_shape):\n",
    "                        grad_arr = grad_arr.reshape(target_shape)\n",
    "                    else:\n",
    "                        avg_grad = np.mean(grad_arr)\n",
    "                        grad_arr = np.full(target_shape, avg_grad)\n",
    "            else:\n",
    "                grad_arr = np.broadcast_to(grad_arr, target_shape)\n",
    "            \n",
    "            grad = Tensor(grad_arr, requires_grad=create_graph, nd_support=self._nd_support)\n",
    "            grad._name = \"reshaped_grad\"\n",
    "        \n",
    "        if self.grad is None:\n",
    "            self.grad = Tensor(np.zeros_like(self._arr), requires_grad=create_graph, nd_support=self._nd_support)\n",
    "            if self._name.startswith('in:'):\n",
    "                self.grad._name = \"in_grad\"\n",
    "            else:\n",
    "                self.grad._name = f\"{self._name}_grad\"\n",
    "        \n",
    "        self.grad = add(self.grad, grad, nd_support=self._nd_support)\n",
    "        if create_graph:\n",
    "            self.grad.requires_grad = True\n",
    "        \n",
    "        if self._operation and self._parents:\n",
    "            parent_grads = self._operation.grad(self.grad, *self._parents)\n",
    "            if len(parent_grads) != len(self._parents):\n",
    "                raise ValueError(f\"Expected {len(self._parents)} gradients, got {len(parent_grads)}\")\n",
    "            \n",
    "            for parent, parent_grad in zip(self._parents, parent_grads):\n",
    "                if parent.requires_grad:\n",
    "                    parent.backward(parent_grad, create_graph=create_graph)\n",
    "    \n",
    "    def compute_second_derivative(self) -> Optional['Tensor']:\n",
    "        \"\"\"\n",
    "        Compute second-order derivative by differentiating the gradient\n",
    "        This is the clean approach for automatic higher-order derivatives\n",
    "        \"\"\"\n",
    "        if self.grad is None:\n",
    "            raise ValueError(\"Must compute first derivative before second derivative\")\n",
    "        \n",
    "        if not self.grad.requires_grad:\n",
    "            print(\"Warning: Gradient does not have computation graph. Computing analytical second derivative if available.\")\n",
    "            \n",
    "            if self._operation and hasattr(self._operation, 'analytical_second_derivative'):\n",
    "                try:\n",
    "                    dummy_grad = Tensor(np.ones_like(self._arr), nd_support=self._nd_support)\n",
    "                    second_grads = self._operation.analytical_second_derivative(dummy_grad, *self._parents)\n",
    "                    if second_grads and len(second_grads) > 0:\n",
    "                        for i, parent in enumerate(self._parents):\n",
    "                            if parent is self or np.array_equal(parent._arr, self._arr):\n",
    "                                return second_grads[i]\n",
    "                        return second_grads[0]\n",
    "                except Exception as e:\n",
    "                    print(f\"Analytical second derivative failed: {e}\")\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            if self.grad._arr.size > 1:\n",
    "                grad_sum = my_sum(self.grad)\n",
    "            else:\n",
    "                grad_sum = self.grad\n",
    "            \n",
    "            first_grad = self.grad.detach()\n",
    "            \n",
    "            self.zero_grad()\n",
    "            \n",
    "            grad_sum.backward()\n",
    "            \n",
    "            self._second_order_grad = self.grad.detach() if self.grad is not None else None\n",
    "            \n",
    "            self.grad = first_grad\n",
    "            \n",
    "            return self._second_order_grad\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Automatic second derivative computation failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def __add__(self, other): return add(self, other, nd_support=self._nd_support)\n",
    "    def __radd__(self, other): return add(other, self, nd_support=self._nd_support)\n",
    "    def __sub__(self, other): return sub(self, other, nd_support=self._nd_support)\n",
    "    def __rsub__(self, other): return sub(other, self, nd_support=self._nd_support)\n",
    "    def __mul__(self, other): return prod(self, other, nd_support=self._nd_support)\n",
    "    def __rmul__(self, other): return prod(other, self, nd_support=self._nd_support)\n",
    "    def __truediv__(self, other): return div(self, other, nd_support=self._nd_support)\n",
    "    def __rtruediv__(self, other): return div(other, self, nd_support=self._nd_support)\n",
    "    def __matmul__(self, other): return matmul(self, other, nd_support=self._nd_support)\n",
    "    def __rmatmul__(self, other): return matmul(other, self, nd_support=self._nd_support)\n",
    "    def __pow__(self, power): return power_op(self, power, nd_support=self._nd_support)\n",
    "    def __neg__(self): return prod(self, -1, nd_support=self._nd_support)\n",
    "    \n",
    "    def sum(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':\n",
    "        return my_sum(self, axis=axis, keepdims=keepdims, nd_support=self._nd_support)\n",
    "    \n",
    "    def mean(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':\n",
    "        return mean(self, axis=axis, keepdims=keepdims, nd_support=self._nd_support)\n",
    "    \n",
    "    def max(self, axis: Optional[Union[int, Tuple[int, ...]]] = None, keepdims: bool = False) -> 'Tensor':\n",
    "        return max_op(self, axis=axis, keepdims=keepdims, nd_support=self._nd_support)\n",
    "    \n",
    "    def reshape(self, *shape) -> 'Tensor':\n",
    "        \"\"\"Reshape tensor\"\"\"\n",
    "        if len(shape) == 1 and isinstance(shape[0], (list, tuple)):\n",
    "            shape = shape[0]\n",
    "        result = Tensor(self._arr.reshape(shape), requires_grad=self.requires_grad, nd_support=self._nd_support)\n",
    "        result._name = f\"{self._name}.reshape{shape}\"\n",
    "        return result\n",
    "    \n",
    "    def squeeze(self, axis: Optional[int] = None) -> 'Tensor':\n",
    "        \"\"\"Remove single-dimensional entries\"\"\"\n",
    "        result = Tensor(np.squeeze(self._arr, axis=axis), requires_grad=self.requires_grad, nd_support=self._nd_support)\n",
    "        result._name = f\"{self._name}.squeeze\"\n",
    "        return result\n",
    "    \n",
    "    def unsqueeze(self, axis: int) -> 'Tensor':\n",
    "        \"\"\"Add single-dimensional entry\"\"\"\n",
    "        result = Tensor(np.expand_dims(self._arr, axis=axis), requires_grad=self.requires_grad, nd_support=self._nd_support)\n",
    "        result._name = f\"{self._name}.unsqueeze\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c612fc",
   "metadata": {},
   "source": [
    "### Interface de  Operações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db44044",
   "metadata": {},
   "source": [
    "A classe abaixo define a interface que as operações devem implementar. Ela não precisa ser modificada, mas pode, caso queira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28a19b73",
   "metadata": {
    "tags": [
     "op"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Op(ABC):\n",
    "    \"\"\"Abstract base class for operations\"\"\"\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        pass\n",
    "\n",
    "\n",
    "def ensure_tensor(x: Any, nd_support: bool = False) -> Tensor:\n",
    "    \"\"\"Convert input to Tensor if not already\"\"\"\n",
    "    if not isinstance(x, Tensor):\n",
    "        x = Tensor(x, nd_support=nd_support)\n",
    "    elif nd_support and not x._nd_support:\n",
    "        x = Tensor(x._arr, requires_grad=x.requires_grad, nd_support=True)\n",
    "    return x\n",
    "\n",
    "\n",
    "def broadcast_shapes(shape1: Tuple[int, ...], shape2: Tuple[int, ...]) -> Tuple[int, ...]:\n",
    "    \"\"\"Compute broadcast shape for two shapes\"\"\"\n",
    "    max_len = max(len(shape1), len(shape2))\n",
    "    shape1 = (1,) * (max_len - len(shape1)) + shape1\n",
    "    shape2 = (1,) * (max_len - len(shape2)) + shape2\n",
    "    result = []\n",
    "    \n",
    "    for s1, s2 in zip(shape1, shape2):\n",
    "        if s1 == s2 or s1 == 1 or s2 == 1:\n",
    "            result.append(max(s1, s2))\n",
    "        else:\n",
    "            raise ValueError(f\"Shapes {shape1} and {shape2} are not broadcastable\")\n",
    "    \n",
    "    return tuple(result)\n",
    "\n",
    "\n",
    "def broadcast_gradient(grad: Tensor, original_shape: Tuple[int, ...], nd_support: bool = False) -> Tensor:\n",
    "    \"\"\"Broadcast gradient back to original shape\"\"\"\n",
    "    grad_arr = grad.numpy()\n",
    "    grad_shape = grad.shape\n",
    "    \n",
    "    if grad_shape == original_shape:\n",
    "        return Tensor(grad_arr, requires_grad=grad.requires_grad, nd_support=nd_support)\n",
    "    \n",
    "    while len(grad_shape) > len(original_shape):\n",
    "        grad_arr = np.sum(grad_arr, axis=0, keepdims=False)\n",
    "        grad_shape = grad_arr.shape\n",
    "    \n",
    "    for i in range(len(grad_shape)):\n",
    "        if i < len(original_shape) and original_shape[i] == 1 and grad_shape[i] > 1:\n",
    "            grad_arr = np.sum(grad_arr, axis=i, keepdims=True)\n",
    "    \n",
    "    if grad_arr.shape != original_shape:\n",
    "        try:\n",
    "            grad_arr = np.reshape(grad_arr, original_shape)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Cannot broadcast gradient shape {grad_arr.shape} to {original_shape}\")\n",
    "    \n",
    "    return Tensor(grad_arr, requires_grad=grad.requires_grad, nd_support=nd_support)\n",
    "\n",
    "class Max(Op):\n",
    "    \"\"\"Max operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Max requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        axis = kwargs.get('axis', None)\n",
    "        keepdims = kwargs.get('keepdims', False)\n",
    "        \n",
    "        result_arr = np.max(a.numpy(), axis=axis, keepdims=keepdims)\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('max')\n",
    "        self._axis = axis\n",
    "        self._keepdims = keepdims\n",
    "        self._input_shape = a.shape\n",
    "        self._max_positions = (a.numpy() == np.max(a.numpy(), axis=axis, keepdims=True))\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        back_grad_arr = back_grad.numpy()\n",
    "        \n",
    "        if self._axis is not None and not self._keepdims:\n",
    "            if isinstance(self._axis, int):\n",
    "                back_grad_arr = np.expand_dims(back_grad_arr, self._axis)\n",
    "            else:\n",
    "                for ax in sorted(self._axis, reverse=True):\n",
    "                    back_grad_arr = np.expand_dims(back_grad_arr, ax)\n",
    "        \n",
    "        grad_arr = self._max_positions.astype(np.float64) * back_grad_arr\n",
    "        \n",
    "        if grad_arr.shape != self._input_shape:\n",
    "            grad_arr = np.broadcast_to(grad_arr, self._input_shape)\n",
    "        \n",
    "        grad_a = Tensor(grad_arr, requires_grad=back_grad.requires_grad, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"max_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "class Exp(Op):\n",
    "    \"\"\"Exponential operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Exp requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        \n",
    "        clipped_arr = np.clip(a.numpy(), -500, 500)\n",
    "        result_arr = np.exp(clipped_arr)\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('exp')\n",
    "        self._output = result\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        grad_a = prod(back_grad, self._output, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"exp_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "    def analytical_second_derivative(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        \"\"\"Analytical second derivative: d²/dx²(exp(x)) = exp(x)\"\"\"\n",
    "        a = args[0]\n",
    "        exp_val = exp(a, nd_support=a._nd_support)\n",
    "        second_grad = prod(back_grad, exp_val, nd_support=a._nd_support)\n",
    "        second_grad._name = f\"exp_second_grad\"\n",
    "        return [second_grad]\n",
    "\n",
    "\n",
    "class Log(Op):\n",
    "    \"\"\"Natural logarithm operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Log requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        \n",
    "        a_arr = a.numpy()\n",
    "        if np.any(a_arr <= 0):\n",
    "            raise ValueError(\"Logarithm of non-positive values detected\")\n",
    "        \n",
    "        result_arr = np.log(a_arr)\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('log')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        grad_a = prod(back_grad, div(Tensor(1, nd_support=a._nd_support), a, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"log_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "    def analytical_second_derivative(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        \"\"\"Analytical second derivative: d²/dx²(log(x)) = -1/x²\"\"\"\n",
    "        a = args[0]\n",
    "        neg_inv_x_sq = div(Tensor(-1, nd_support=a._nd_support), power_op(a, 2, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        second_grad = prod(back_grad, neg_inv_x_sq, nd_support=a._nd_support)\n",
    "        second_grad._name = f\"log_second_grad\"\n",
    "        return [second_grad]\n",
    "    \n",
    "class Power(Op):\n",
    "    \"\"\"Power operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 2, \"Power requires exactly 2 arguments\"\n",
    "        a, power = ensure_tensor(args[0], kwargs.get('nd_support', False)), ensure_tensor(args[1], kwargs.get('nd_support', False))\n",
    "        \n",
    "        if not a._nd_support and power.shape not in ((), (1,), (1, 1)):\n",
    "            raise ValueError(f\"Power must be scalar, got shape {power.shape}\")\n",
    "        \n",
    "        power_val = power.item() if power._arr.size == 1 else power.numpy()\n",
    "        result_arr = np.power(a.numpy(), power_val)\n",
    "        result = Tensor(result_arr, parents=[a, power], operation=self,\n",
    "                       requires_grad=a.requires_grad or power.requires_grad,\n",
    "                       nd_support=a._nd_support or power._nd_support)\n",
    "        result._name = NameManager.new('power')\n",
    "        self._power_val = power_val\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a, power = args\n",
    "        power_val = self._power_val\n",
    "        \n",
    "        if isinstance(power_val, np.ndarray):\n",
    "            power_minus_one = power_val - 1\n",
    "            grad_a = prod(back_grad, prod(power, power_op(a, power_minus_one, nd_support=a._nd_support), nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        else:\n",
    "            grad_a = prod(back_grad, Tensor(power_val * np.power(a.numpy(), power_val - 1), nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        \n",
    "        grad_power = Tensor(np.zeros_like(power.numpy()), nd_support=power._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"power_grad\"\n",
    "        \n",
    "        if power._name.startswith('in:'):\n",
    "            grad_power._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_power._name = f\"power_grad\"\n",
    "        \n",
    "        return [grad_a, grad_power]\n",
    "    \n",
    "    def analytical_second_derivative(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        \"\"\"Analytical second derivative: d²/dx²(x^n) = n(n-1)x^(n-2)\"\"\"\n",
    "        a, power = args\n",
    "        power_val = self._power_val\n",
    "        \n",
    "        if isinstance(power_val, (int, float)) and power_val >= 2:\n",
    "            second_coeff = power_val * (power_val - 1)\n",
    "            if power_val == 2:\n",
    "                second_grad = prod(back_grad, Tensor(second_coeff * np.ones_like(a._arr), nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "            else:\n",
    "                second_grad = prod(back_grad, Tensor(second_coeff * np.power(a.numpy(), power_val - 2), nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        else:\n",
    "            second_grad = Tensor(np.zeros_like(a._arr), nd_support=a._nd_support)\n",
    "        \n",
    "        second_grad_power = Tensor(np.zeros_like(power.numpy()), nd_support=power._nd_support)\n",
    "        \n",
    "        second_grad._name = f\"power_second_grad\"\n",
    "        second_grad_power._name = f\"power_second_grad\"\n",
    "        return [second_grad, second_grad_power]\n",
    "    \n",
    "class Div(Op):\n",
    "    \"\"\"Division operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 2, \"Div requires exactly 2 arguments\"\n",
    "        a, b = ensure_tensor(args[0], kwargs.get('nd_support', False)), ensure_tensor(args[1], kwargs.get('nd_support', False))\n",
    "        \n",
    "        try:\n",
    "            output_shape = broadcast_shapes(a.shape, b.shape)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Div: {e}\")\n",
    "        \n",
    "        b_arr = b.numpy()\n",
    "        if np.any(np.abs(b_arr) < 1e-8):\n",
    "            raise ValueError(\"Division by zero detected\")\n",
    "        \n",
    "        result_arr = np.divide(a.numpy(), b_arr, dtype=np.float64)\n",
    "        result = Tensor(result_arr, parents=[a, b], operation=self,\n",
    "                       requires_grad=a.requires_grad or b.requires_grad,\n",
    "                       nd_support=a._nd_support or b._nd_support)\n",
    "        result._name = NameManager.new('div')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a, b = args\n",
    "        grad_a_intermediate = div(back_grad, b, nd_support=a._nd_support)\n",
    "        grad_a = broadcast_gradient(grad_a_intermediate, a.shape, a._nd_support)\n",
    "        \n",
    "        neg_a = Tensor(-a.numpy(), nd_support=b._nd_support)\n",
    "        b_squared = power_op(b, 2, nd_support=b._nd_support)\n",
    "        grad_b_intermediate = prod(back_grad, div(neg_a, b_squared, nd_support=b._nd_support), nd_support=b._nd_support)\n",
    "        grad_b = broadcast_gradient(grad_b_intermediate, b.shape, b._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"div_grad\"\n",
    "        \n",
    "        if b._name.startswith('in:'):\n",
    "            grad_b._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_b._name = f\"div_grad\"\n",
    "        \n",
    "        return [grad_a, grad_b]    \n",
    "    \n",
    "div = Div()\n",
    "power_op = Power()\n",
    "exp = Exp()\n",
    "log = Log()\n",
    "max_op = Max()\n",
    "\n",
    "\n",
    "def square(x: Tensor, nd_support: bool = False) -> Tensor:\n",
    "    \"\"\"Square function using power operation\"\"\"\n",
    "    return power_op(x, Tensor(2, nd_support=nd_support), nd_support=nd_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89e386",
   "metadata": {},
   "source": [
    "### Implementação das Operações\n",
    "\n",
    "Operações devem herdar de `Op` e implementar os métodos `__call__` e `grad`.\n",
    "\n",
    "Pelo menos as seguintes operações devem ser implementadas:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4f7719",
   "metadata": {
    "tags": [
     "add"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Add(Op):\n",
    "    \"\"\"Addition operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 2, \"Add requires exactly 2 arguments\"\n",
    "        a, b = ensure_tensor(args[0], kwargs.get('nd_support', False)), ensure_tensor(args[1], kwargs.get('nd_support', False))\n",
    "        \n",
    "        try:\n",
    "            output_shape = broadcast_shapes(a.shape, b.shape)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Add: {e}\")\n",
    "        \n",
    "        result_arr = np.add(a.numpy(), b.numpy(), dtype=np.float64)\n",
    "        result = Tensor(result_arr, parents=[a, b], operation=self, \n",
    "                       requires_grad=a.requires_grad or b.requires_grad,\n",
    "                       nd_support=a._nd_support or b._nd_support)\n",
    "        result._name = NameManager.new('add')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a, b = args\n",
    "        grad_a = broadcast_gradient(back_grad, a.shape, a._nd_support)\n",
    "        grad_b = broadcast_gradient(back_grad, b.shape, b._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"add_grad\"\n",
    "        \n",
    "        if b._name.startswith('in:'):\n",
    "            grad_b._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_b._name = f\"add_grad\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "add = Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05cb44e6",
   "metadata": {
    "tags": [
     "sub"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sub(Op):\n",
    "    \"\"\"Subtraction operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 2, \"Sub requires exactly 2 arguments\"\n",
    "        a, b = ensure_tensor(args[0], kwargs.get('nd_support', False)), ensure_tensor(args[1], kwargs.get('nd_support', False))\n",
    "        \n",
    "        try:\n",
    "            output_shape = broadcast_shapes(a.shape, b.shape)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Sub: {e}\")\n",
    "        \n",
    "        result_arr = np.subtract(a.numpy(), b.numpy(), dtype=np.float64)\n",
    "        result = Tensor(result_arr, parents=[a, b], operation=self,\n",
    "                       requires_grad=a.requires_grad or b.requires_grad,\n",
    "                       nd_support=a._nd_support or b._nd_support)\n",
    "        result._name = NameManager.new('sub')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a, b = args\n",
    "        grad_a = broadcast_gradient(back_grad, a.shape, a._nd_support)\n",
    "        neg_grad = Tensor(-back_grad.numpy(), nd_support=b._nd_support)\n",
    "        grad_b = broadcast_gradient(neg_grad, b.shape, b._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"sub_grad\"\n",
    "        \n",
    "        if b._name.startswith('in:'):\n",
    "            grad_b._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_b._name = f\"sub_grad\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sub = Sub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f53df08",
   "metadata": {
    "tags": [
     "prod"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Prod(Op):\n",
    "    \"\"\"Element-wise multiplication operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 2, \"Prod requires exactly 2 arguments\"\n",
    "        a, b = ensure_tensor(args[0], kwargs.get('nd_support', False)), ensure_tensor(args[1], kwargs.get('nd_support', False))\n",
    "        \n",
    "        try:\n",
    "            output_shape = broadcast_shapes(a.shape, b.shape)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Prod: {e}\")\n",
    "        \n",
    "        result_arr = np.multiply(a.numpy(), b.numpy(), dtype=np.float64)\n",
    "        result = Tensor(result_arr, parents=[a, b], operation=self,\n",
    "                       requires_grad=a.requires_grad or b.requires_grad,\n",
    "                       nd_support=a._nd_support or b._nd_support)\n",
    "        result._name = NameManager.new('prod')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a, b = args\n",
    "        grad_a_intermediate = prod(back_grad, b, nd_support=a._nd_support)\n",
    "        grad_b_intermediate = prod(back_grad, a, nd_support=b._nd_support)\n",
    "        grad_a = broadcast_gradient(grad_a_intermediate, a.shape, a._nd_support)\n",
    "        grad_b = broadcast_gradient(grad_b_intermediate, b.shape, b._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"prod_grad\"\n",
    "        \n",
    "        if b._name.startswith('in:'):\n",
    "            grad_b._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_b._name = f\"prod_grad\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "prod = Prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3838a7",
   "metadata": {
    "tags": [
     "sin"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sin(Op):\n",
    "    \"\"\"Sine operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Sin requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        result_arr = np.sin(a.numpy())\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('sin')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        grad_a = prod(back_grad, cos(a, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"sin_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "    def analytical_second_derivative(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        \"\"\"Analytical second derivative: d²/dx²(sin(x)) = -sin(x)\"\"\"\n",
    "        a = args[0]\n",
    "        neg_sin = prod(Tensor(-1, nd_support=a._nd_support), sin(a, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        second_grad = prod(back_grad, neg_sin, nd_support=a._nd_support)\n",
    "        second_grad._name = f\"sin_second_grad\"\n",
    "        return [second_grad]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sin = Sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138cb8ef",
   "metadata": {
    "tags": [
     "cos"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Cos(Op):\n",
    "    \"\"\"Cosine operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Cos requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        result_arr = np.cos(a.numpy())\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('cos')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        neg_sin = prod(Tensor(-1, nd_support=a._nd_support), sin(a, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        grad_a = prod(back_grad, neg_sin, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"cos_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "    def analytical_second_derivative(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        \"\"\"Analytical second derivative: d²/dx²(cos(x)) = -cos(x)\"\"\"\n",
    "        a = args[0]\n",
    "        neg_cos = prod(Tensor(-1, nd_support=a._nd_support), cos(a, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        second_grad = prod(back_grad, neg_cos, nd_support=a._nd_support)\n",
    "        second_grad._name = f\"cos_second_grad\"\n",
    "        return [second_grad]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "cos = Cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46eac52c",
   "metadata": {
    "tags": [
     "sum"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sum(Op):\n",
    "    \"\"\"Sum operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Sum requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        axis = kwargs.get('axis', None)\n",
    "        keepdims = kwargs.get('keepdims', False)\n",
    "        \n",
    "        result_arr = np.sum(a.numpy(), axis=axis, keepdims=keepdims, dtype=np.float64)\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('sum')\n",
    "        self._axis = axis\n",
    "        self._keepdims = keepdims\n",
    "        self._input_shape = a.shape\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        back_grad_arr = back_grad.numpy()\n",
    "        \n",
    "        if self._axis is not None and not self._keepdims:\n",
    "            if isinstance(self._axis, int):\n",
    "                back_grad_arr = np.expand_dims(back_grad_arr, self._axis)\n",
    "            else:\n",
    "                for ax in sorted(self._axis, reverse=True):\n",
    "                    back_grad_arr = np.expand_dims(back_grad_arr, ax)\n",
    "        \n",
    "        grad_arr = np.broadcast_to(back_grad_arr, self._input_shape)\n",
    "        grad_a = Tensor(grad_arr, requires_grad=back_grad.requires_grad, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"sum_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "# ⚠️ vamos chamar de my_sum porque python ja possui uma funcao sum\n",
    "my_sum = Sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e098a39c",
   "metadata": {
    "tags": [
     "mean"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Mean(Op):\n",
    "    \"\"\"Mean operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Mean requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        axis = kwargs.get('axis', None)\n",
    "        keepdims = kwargs.get('keepdims', False)\n",
    "        \n",
    "        result_arr = np.mean(a.numpy(), axis=axis, keepdims=keepdims, dtype=np.float64)\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('mean')\n",
    "        self._axis = axis\n",
    "        self._keepdims = keepdims\n",
    "        self._input_shape = a.shape\n",
    "        \n",
    "        if axis is None:\n",
    "            self._size = a._arr.size\n",
    "        else:\n",
    "            axes = axis if isinstance(axis, tuple) else (axis,)\n",
    "            self._size = np.prod([a.shape[i] for i in axes])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        back_grad_arr = back_grad.numpy()\n",
    "        \n",
    "        if self._axis is not None and not self._keepdims:\n",
    "            if isinstance(self._axis, int):\n",
    "                back_grad_arr = np.expand_dims(back_grad_arr, self._axis)\n",
    "            else:\n",
    "                for ax in sorted(self._axis, reverse=True):\n",
    "                    back_grad_arr = np.expand_dims(back_grad_arr, ax)\n",
    "        \n",
    "        grad_arr = np.broadcast_to(back_grad_arr, self._input_shape) / self._size\n",
    "        grad_a = Tensor(grad_arr, requires_grad=back_grad.requires_grad, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"mean_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "mean = Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37692879",
   "metadata": {
    "tags": [
     "square"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Square(Op):\n",
    "    \"\"\"Square operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Square requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        result_arr = np.square(a.numpy())\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('square')\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        grad_a = prod(back_grad, prod(Tensor(2, nd_support=a._nd_support), a, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"square_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "    def analytical_second_derivative(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        \"\"\"Analytical second derivative: d²/dx²(x²) = 2\"\"\"\n",
    "        a = args[0]\n",
    "        second_grad = prod(back_grad, Tensor(2 * np.ones_like(a._arr), nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        second_grad._name = f\"square_second_grad\"\n",
    "        return [second_grad]\n",
    "\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "square = Square()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6542807d",
   "metadata": {
    "tags": [
     "matmul"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class MatMul(Op):\n",
    "    \"\"\"Matrix multiplication operation with correct gradient computation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 2, \"MatMul requires exactly 2 arguments\"\n",
    "        a, b = ensure_tensor(args[0], kwargs.get('nd_support', False)), ensure_tensor(args[1], kwargs.get('nd_support', False))\n",
    "        \n",
    "        a_arr = a.numpy()\n",
    "        b_arr = b.numpy()\n",
    "        \n",
    "        self._a_original_shape = a.shape\n",
    "        self._b_original_shape = b.shape\n",
    "        \n",
    "        a_was_scalar = a_arr.size == 1\n",
    "        if a_was_scalar and a_arr.ndim == 2 and a_arr.shape == (1, 1):\n",
    "            pass\n",
    "        elif a_arr.ndim == 1:\n",
    "            a_arr = a_arr.reshape(-1, 1)\n",
    "        \n",
    "        b_was_1d = b_arr.ndim == 1\n",
    "        if b_was_1d:\n",
    "            b_arr = b_arr.reshape(-1, 1)\n",
    "        \n",
    "        self._a_was_scalar = a_was_scalar\n",
    "        self._b_was_1d = b_was_1d\n",
    "        \n",
    "        if a_arr.ndim < 2:\n",
    "            a_arr = a_arr.reshape(1, -1) if a_arr.size > 1 else a_arr.reshape(1, 1)\n",
    "        if b_arr.ndim < 2:\n",
    "            b_arr = b_arr.reshape(-1, 1) if b_arr.size > 1 else b_arr.reshape(1, 1)\n",
    "        \n",
    "        if a_arr.shape[-1] != b_arr.shape[0]:\n",
    "            raise ValueError(f\"Incompatible shapes for matmul: {a.shape} -> {a_arr.shape} @ {b.shape} -> {b_arr.shape}\")\n",
    "        \n",
    "        result_arr = np.matmul(a_arr, b_arr)\n",
    "        \n",
    "        self._a_comp_shape = a_arr.shape\n",
    "        self._b_comp_shape = b_arr.shape\n",
    "        \n",
    "        result = Tensor(result_arr, parents=[a, b], operation=self,\n",
    "                       requires_grad=a.requires_grad or b.requires_grad,\n",
    "                       nd_support=a._nd_support or b._nd_support)\n",
    "        result._name = NameManager.new('matmul')\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a, b = args\n",
    "        back_grad_arr = back_grad.numpy()\n",
    "        \n",
    "        if back_grad_arr.ndim == 1:\n",
    "            back_grad_arr = back_grad_arr.reshape(-1, 1)\n",
    "        elif back_grad_arr.ndim == 0:\n",
    "            back_grad_arr = back_grad_arr.reshape(1, 1)\n",
    "        \n",
    "        a_arr = a.numpy()\n",
    "        b_arr = b.numpy()\n",
    "        \n",
    "        if self._a_was_scalar and a_arr.shape == (1, 1):\n",
    "            pass  \n",
    "        elif a_arr.ndim == 1:\n",
    "            a_arr = a_arr.reshape(-1, 1)\n",
    "        if a_arr.ndim < 2:\n",
    "            a_arr = a_arr.reshape(1, -1) if a_arr.size > 1 else a_arr.reshape(1, 1)\n",
    "            \n",
    "        if self._b_was_1d:\n",
    "            b_arr = b_arr.reshape(-1, 1)\n",
    "        if b_arr.ndim < 2:\n",
    "            b_arr = b_arr.reshape(-1, 1) if b_arr.size > 1 else b_arr.reshape(1, 1)\n",
    "        \n",
    "        try:\n",
    "            b_transpose = np.transpose(b_arr)\n",
    "            \n",
    "            if back_grad_arr.shape[1] != b_transpose.shape[0]:\n",
    "                if back_grad_arr.size == b_transpose.shape[0]:\n",
    "                    back_grad_arr = back_grad_arr.reshape(1, -1)\n",
    "                elif back_grad_arr.shape[0] == b_transpose.shape[0]:\n",
    "                    back_grad_arr = back_grad_arr.T\n",
    "            \n",
    "            grad_a_arr = np.matmul(back_grad_arr, b_transpose)\n",
    "            \n",
    "            a_transpose = np.transpose(a_arr)\n",
    "            \n",
    "            if a_transpose.shape[1] != back_grad_arr.shape[0]:\n",
    "                if a_transpose.shape[0] == back_grad_arr.shape[0]:\n",
    "                    grad_b_arr = np.matmul(a_transpose.T, back_grad_arr)\n",
    "                elif a_transpose.size == back_grad_arr.size:\n",
    "                    grad_b_arr = a_transpose * back_grad_arr\n",
    "                else:\n",
    "                    total_grad = np.sum(back_grad_arr)\n",
    "                    grad_b_arr = np.full(b_arr.shape, total_grad / b_arr.size)\n",
    "            else:\n",
    "                grad_b_arr = np.matmul(a_transpose, back_grad_arr)\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"MatMul gradient fallback triggered: {e}\")\n",
    "            \n",
    "            total_grad = np.sum(back_grad_arr)\n",
    "            grad_a_arr = np.full(self._a_comp_shape, total_grad / np.prod(self._a_comp_shape))\n",
    "            grad_b_arr = np.full(self._b_comp_shape, total_grad / np.prod(self._b_comp_shape))\n",
    "        \n",
    "        try:\n",
    "            if grad_a_arr.shape != self._a_original_shape:\n",
    "                if grad_a_arr.size == np.prod(self._a_original_shape):\n",
    "                    grad_a_arr = grad_a_arr.reshape(self._a_original_shape)\n",
    "                elif grad_a_arr.size > np.prod(self._a_original_shape):\n",
    "                    grad_a_arr = np.sum(grad_a_arr) / np.prod(self._a_original_shape)\n",
    "                    grad_a_arr = np.full(self._a_original_shape, grad_a_arr)\n",
    "                else:\n",
    "                    grad_a_arr = np.full(self._a_original_shape, np.mean(grad_a_arr))\n",
    "            \n",
    "            if grad_b_arr.shape != self._b_original_shape:\n",
    "                if grad_b_arr.size == np.prod(self._b_original_shape):\n",
    "                    grad_b_arr = grad_b_arr.reshape(self._b_original_shape)\n",
    "                elif grad_b_arr.size > np.prod(self._b_original_shape):\n",
    "                    grad_b_arr = np.sum(grad_b_arr) / np.prod(self._b_original_shape)\n",
    "                    grad_b_arr = np.full(self._b_original_shape, grad_b_arr)\n",
    "                else:\n",
    "                    grad_b_arr = np.full(self._b_original_shape, np.mean(grad_b_arr))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"MatMul gradient reshape failed: {e}\")\n",
    "            grad_a_arr = np.zeros(self._a_original_shape)\n",
    "            grad_b_arr = np.zeros(self._b_original_shape)\n",
    "        \n",
    "        grad_a = Tensor(grad_a_arr, nd_support=a._nd_support)\n",
    "        grad_b = Tensor(grad_b_arr, nd_support=b._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"matmul_grad\"\n",
    "        \n",
    "        if b._name.startswith('in:'):\n",
    "            grad_b._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_b._name = f\"matmul_grad\"\n",
    "        \n",
    "        return [grad_a, grad_b]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "matmul = MatMul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c08a38e3",
   "metadata": {
    "tags": [
     "exp"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Exp(Op):\n",
    "    \"\"\"Exponential operation\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Exp requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        \n",
    "        clipped_arr = np.clip(a.numpy(), -500, 500)\n",
    "        result_arr = np.exp(clipped_arr)\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('exp')\n",
    "        self._output = result\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        grad_a = prod(back_grad, self._output, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"exp_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "    \n",
    "    def analytical_second_derivative(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        \"\"\"Analytical second derivative: d²/dx²(exp(x)) = exp(x)\"\"\"\n",
    "        a = args[0]\n",
    "        exp_val = exp(a, nd_support=a._nd_support)\n",
    "        second_grad = prod(back_grad, exp_val, nd_support=a._nd_support)\n",
    "        second_grad._name = f\"exp_second_grad\"\n",
    "        return [second_grad]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "exp = Exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1acc813e",
   "metadata": {
    "tags": [
     "relu"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReLU(Op):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"ReLU requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        \n",
    "        result_arr = np.maximum(0, a.numpy())\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('relu')\n",
    "        self._input = a\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        mask = (a.numpy() > 0).astype(np.float64)\n",
    "        grad_a = prod(back_grad, Tensor(mask, nd_support=a._nd_support), nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"relu_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "relu = ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae499275",
   "metadata": {
    "tags": [
     "sigmoid"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid(Op):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Sigmoid requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        \n",
    "        clipped_arr = np.clip(a.numpy(), -500, 500)\n",
    "        result_arr = 1 / (1 + np.exp(-clipped_arr))\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('sigmoid')\n",
    "        self._output = result\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        sig = self._output\n",
    "        one_minus_sig = sub(Tensor(1, nd_support=sig._nd_support), sig, nd_support=sig._nd_support)\n",
    "        sig_grad = prod(sig, one_minus_sig, nd_support=sig._nd_support)\n",
    "        grad_a = prod(back_grad, sig_grad, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"sigmoid_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "sigmoid = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce464ba",
   "metadata": {
    "tags": [
     "tanh"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tanh(Op):\n",
    "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Tanh requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        \n",
    "        result_arr = np.tanh(a.numpy())\n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('tanh')\n",
    "        self._output = result\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        tanh_out = self._output\n",
    "        tanh_squared = power_op(tanh_out, 2, nd_support=tanh_out._nd_support)\n",
    "        one_minus_tanh_sq = sub(Tensor(1, nd_support=tanh_out._nd_support), tanh_squared, nd_support=tanh_out._nd_support)\n",
    "        grad_a = prod(back_grad, one_minus_tanh_sq, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"tanh_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "tanh = Tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f15a37eb",
   "metadata": {
    "tags": [
     "softmax"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class Softmax(Op):\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    def __call__(self, *args, **kwargs) -> Tensor:\n",
    "        assert len(args) == 1, \"Softmax requires exactly 1 argument\"\n",
    "        a = ensure_tensor(args[0], kwargs.get('nd_support', False))\n",
    "        \n",
    "        x = a.numpy()\n",
    "        self._input_shape = x.shape\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        elif x.shape[1] != 1 and not a._nd_support:\n",
    "            raise ValueError(\"Softmax expects column vector or 1D array in non-nd_support mode\")\n",
    "        \n",
    "        x_max = np.max(x, axis=0, keepdims=True)\n",
    "        exp_x = np.exp(x - x_max)\n",
    "        sum_exp_x = np.sum(exp_x, axis=0, keepdims=True)\n",
    "        result_arr = exp_x / sum_exp_x\n",
    "        \n",
    "        if result_arr.shape != self._input_shape:\n",
    "            result_arr = result_arr.reshape(self._input_shape)\n",
    "        \n",
    "        result = Tensor(result_arr, parents=[a], operation=self,\n",
    "                       requires_grad=a.requires_grad, nd_support=a._nd_support)\n",
    "        result._name = NameManager.new('softmax')\n",
    "        self._output = result\n",
    "        return result\n",
    "    \n",
    "    def grad(self, back_grad: Tensor, *args, **kwargs) -> List[Tensor]:\n",
    "        a = args[0]\n",
    "        y = self._output.numpy()\n",
    "        back_grad_arr = back_grad.numpy()\n",
    "        \n",
    "        if y.shape != back_grad_arr.shape:\n",
    "            raise ValueError(f\"Gradient shape {back_grad_arr.shape} doesn't match output shape {y.shape}\")\n",
    "        \n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "            back_grad_arr = back_grad_arr.reshape(-1, 1)\n",
    "        elif y.ndim == 2 and y.shape[1] == 1:\n",
    "            pass  \n",
    "        \n",
    "        y_dot_grad = y * back_grad_arr\n",
    "        sum_y_dot_grad = np.sum(y_dot_grad, axis=0, keepdims=True)\n",
    "        grad_a_arr = y * (back_grad_arr - sum_y_dot_grad)\n",
    "        \n",
    "        if grad_a_arr.shape != self._input_shape:\n",
    "            grad_a_arr = grad_a_arr.reshape(self._input_shape)\n",
    "        \n",
    "        grad_a = Tensor(grad_a_arr, requires_grad=back_grad.requires_grad, nd_support=a._nd_support)\n",
    "        \n",
    "        if a._name.startswith('in:'):\n",
    "            grad_a._name = \"in_grad\"\n",
    "        else:\n",
    "            grad_a._name = f\"softmax_grad\"\n",
    "        \n",
    "        return [grad_a]\n",
    "\n",
    "# Instancia a classe. O objeto passa a poder ser usado como uma funcao\n",
    "softmax = Softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12256fd7",
   "metadata": {},
   "source": [
    "\n",
    "### ‼️ Regras e Pontos de Atenção‼️\n",
    "\n",
    "- Vamos fazer a hipótese simplificadora que Tensores devem ser sempre matrizes. Por exemplo, o escalar 2 deve ser armazado em `_arr` como a matriz `[[2]]`. De forma similar, a lista `[1, 2, 3]` deve ser armazenada em `_arr` como em uma matriz coluna.\n",
    "\n",
    "- Devem ser realizados `asserts` nas operações para garantir que os shapes dos operandos fazem sentido. Esta verificação também deve ser feita depois das operações que manipulam gradientes de tensores.\n",
    "\n",
    "- Devem ser respeitados os nomes dos atributos, métodos e classes para viabilizar os testes automáticos.\n",
    "\n",
    "- Gradientes devem ser calculados usando uma passada pelo grafo computacional.\n",
    "\n",
    "- Os gradientes devem ser somados e não substituídos nas chamadas de  backward. Isto vai permitir que os gradientes sejam acumulados entre amostras do dataset e que os resultados sejam corretos mesmo em caso de ramificações e junções no grafo computacional.\n",
    "\n",
    "- Lembre-se de zerar os gradientes após cada passo de gradient descent (atualização dos parâmetros).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc927248",
   "metadata": {},
   "source": [
    "## Testes Básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae08a8",
   "metadata": {},
   "source": [
    "Estes testes avaliam se a derivada da função está sendo calculada corretamente, mas em muitos casos **não** avaliam se os gradientes backpropagados estão sendo incorporados corretamente. Esta avaliação será feita nos problemas da próxima seção."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05318a9",
   "metadata": {},
   "source": [
    "Operador de Soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fd20550",
   "metadata": {
    "tags": [
     "test_add"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([1. 1. 1.], name=add:4, shape=(3, 1))\n",
      "Tensor([1. 1. 1.], name=add:5, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# add\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = add(a, b)\n",
    "d = add(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac72b1a",
   "metadata": {},
   "source": [
    "Operador de Subtração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "612377aa",
   "metadata": {
    "tags": [
     "test_sub"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([1. 1. 1.], name=add:9, shape=(3, 1))\n",
      "Tensor([-1. -1. -1.], name=add:10, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sub\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = sub(a, b)\n",
    "d = sub(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: matrizes coluna contendo 1 e -1\n",
    "print(a.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c8e63",
   "metadata": {},
   "source": [
    "Operador de Produto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc60de82",
   "metadata": {
    "tags": [
     "test_prod"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([12. 15. 18.], name=add:14, shape=(3, 1))\n",
      "Tensor([3. 6. 9.], name=add:15, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# prod\n",
    "\n",
    "a = Tensor([1.0, 2.0, 3.0])\n",
    "b = Tensor([4.0, 5.0, 6.0])\n",
    "c = prod(a, b)\n",
    "d = prod(c, 3.0)\n",
    "d.backward()\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(a.grad)\n",
    "# esperado: [3, 6, 9]^T\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e1c3",
   "metadata": {},
   "source": [
    "Operadores trigonométricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6185a989",
   "metadata": {
    "tags": [
     "test_sin_cos"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([-1.  1. -1.], name=add:23, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# sin e cos\n",
    "\n",
    "a = Tensor([np.pi, 0, np.pi/2])\n",
    "b = sin(a)\n",
    "c = cos(a)\n",
    "d = my_sum(add(b, c))\n",
    "d.backward()\n",
    "\n",
    "# esperado: [-1, 1, -1]^T\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f29f232",
   "metadata": {
    "tags": [
     "test_sum"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([4. 4. 4. 4.], name=add:30, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sum\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = add(prod(a, 3.0), a)\n",
    "c = my_sum(b)\n",
    "c.backward()\n",
    "\n",
    "# esperado: [4, 4, 4, 4]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8943e71a",
   "metadata": {
    "tags": [
     "test_mean"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([0.25 0.25 0.25 0.25], name=add:32, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Mean\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = mean(a)\n",
    "b.backward()\n",
    "\n",
    "# esperado: [0.25, 0.25, 0.25, 0.25]^T\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c7dbd2c",
   "metadata": {
    "tags": [
     "test_square"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([9. 1. 0. 4.], name=square:0, shape=(4, 1))\n",
      "Tensor([6. 2. 0. 4.], name=add:34, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Square\n",
    "\n",
    "a = Tensor([3.0, 1.0, 0.0, 2.0])\n",
    "b = square(a)\n",
    "\n",
    "# esperado: [9, 1, 0, 4]^T\n",
    "print(b)\n",
    "\n",
    "b.backward()\n",
    "\n",
    "# esperado: [6, 2, 0, 4]\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02f2ead7",
   "metadata": {
    "tags": [
     "test_matmul"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([14. 32. 50.], name=matmul:0, shape=(3, 1))\n",
      "Tensor([[1. 2. 3.]\n",
      " [1. 2. 3.]\n",
      " [1. 2. 3.]], name=add:36, shape=(3, 3))\n",
      "Tensor([12. 15. 18.], name=add:37, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# matmul\n",
    "\n",
    "W = Tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0]\n",
    "])\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "z = matmul(W, v)\n",
    "\n",
    "# esperado: [14, 32, 50]^T\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "# esperado:\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "# [1, 2, 3]\n",
    "print(W.grad)\n",
    "\n",
    "# esperado: [12, 15, 18]^T\n",
    "print(v.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "706212d2",
   "metadata": {
    "tags": [
     "test_exp"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([ 2.71828183  7.3890561  20.08553692], name=exp:0, shape=(3, 1))\n",
      "Tensor([ 2.71828183  7.3890561  20.08553692], name=add:39, shape=(3, 1))\n"
     ]
    }
   ],
   "source": [
    "# Exp\n",
    "\n",
    "v = Tensor([1.0, 2.0, 3.0])\n",
    "w = exp(v)\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [2.718..., 7.389..., 20.085...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9510d010",
   "metadata": {
    "tags": [
     "test_relu"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([0. 0. 1. 3.], name=relu:0, shape=(4, 1))\n",
      "Tensor([0. 0. 1. 1.], name=add:41, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Relu\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = relu(v)\n",
    "\n",
    "# esperado: [0, 0, 1, 3]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0, 0, 1, 1]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f0fbf8d",
   "metadata": {
    "tags": [
     "test_sigmoid"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([0.26894142 0.5        0.73105858 0.95257413], name=sigmoid:0, shape=(4, 1))\n",
      "Tensor([0.19661193 0.25       0.19661193 0.04517666], name=add:43, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = sigmoid(v)\n",
    "\n",
    "# esperado: [0.268.., 0.5, 0.731.., 0.952..]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.196..., 0.25, 0.196..., 0.045...]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e867dec",
   "metadata": {
    "tags": [
     "test_tanh"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([-0.76159416  0.          0.76159416  0.99505475], name=tanh:0, shape=(4, 1))\n",
      "Tensor([0.41997434 1.         0.41997434 0.00986604], name=add:45, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Tanh\n",
    "\n",
    "v = Tensor([-1.0, 0.0, 1.0, 3.0])\n",
    "w = tanh(v)\n",
    "\n",
    "# esperado: [[-0.76159416, 0., 0.76159416, 0.99505475]^T\n",
    "print(w)\n",
    "\n",
    "w.backward()\n",
    "\n",
    "# esperado: [0.41997434, 1., 0.41997434, 0.00986604]^T\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7fd3235d",
   "metadata": {
    "tags": [
     "test_softmax"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([0.00381737 0.13970902 0.23034123 0.62613238], name=softmax:0, shape=(4, 1))\n",
      "MSE: Tensor([[0.36424932]], name=mean:1, shape=(1, 1))\n",
      "Tensor([-0.00278095 -0.02243068 -0.02654377  0.05175539], name=add:50, shape=(4, 1))\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "\n",
    "x = Tensor([-3.1, 0.5, 1.0, 2.0])\n",
    "y = softmax(x)\n",
    "\n",
    "# esperado: [0.00381737, 0.13970902, 0.23034123, 0.62613238]^T\n",
    "print(y)\n",
    "\n",
    "# como exemplo, calcula o MSE para um target vector\n",
    "diff = sub(y, [1, 0, 0, 0])\n",
    "sq = square(diff)\n",
    "a = mean(sq)\n",
    "\n",
    "# esperado: 0.36424932\n",
    "print(\"MSE:\", a)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "# esperado: [-0.00278095, -0.02243068, -0.02654377, 0.05175539]^T\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a3d9f",
   "metadata": {},
   "source": [
    "## Pontos Extras\n",
    "\n",
    "### Tarefas\n",
    "\n",
    "- **+2 pontos**: Utilizar sobrecarga de operadores para permitir que todas as operações disponíveis aos arrays do numpy possam ser realizadas com tensores, incluindo operações que envolvam broadcasting.\n",
    "  - Por exemplo, assumindo que a e b são tensores possivelmente com dimensões diferentes, devem ser possível realizar as operações a + 2, a * b, a @ b, a.max(), a.sum(axis=0).\n",
    "  - Para realizar esta atividade, os atributos da classe Tensor podem ser completamente modificados, mas deve ser provido um método backward para iniciar o backpropagation.\n",
    "  - Naturalmente, a regra de que tensores devem ser matrizes deve ser desconsiderada neste caso.\n",
    "\n",
    "- **+1 ponto**: Atualizar as classes para permitir derivadas de mais alta ordem (derivadas segundas, etc.).\n",
    "\n",
    "- **+1 ponto**: Entregar uma versão adicional do trabalho completo usando C/C++ e com foco em minimizar o tempo para realização das operações. Os casos de teste do sistema Testr também deverão ser replicados utilizando esta linguagem.\n",
    "\n",
    "### Regras\n",
    "\n",
    "- Só serão elegíveis para receber pontos extras os alunos que cumprirem 100% dos requisitos da parte principal do trabalho.\n",
    "\n",
    "- Para receber os pontos extras, deverá ser agendado um horário para uma entrevista individual que abordará tanto os códigos-fonte relativos aos pontos extras quanto à parte principal do trabalho (pode acontecer redução da pontuação da parte principal do trabalho).\n",
    "\n",
    "- Receberá os pontos extras quem responder corretamente às perguntas da entrevista. Não será atribuída pontuação parcial aos pontos extras.\n",
    "\n",
    "## Referências\n",
    "\n",
    "### Principais\n",
    "\n",
    "- [Build your own pytorch](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-1-Computation-Graphs/)\n",
    "- [Build your own Pytorch - 2: Backpropagation](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-2-Autograd/)\n",
    "- [Build your own PyTorch - 3: Training a Neural Network with self-made AD software](https://www.peterholderrieth.com/blog/2023/Build-Your-Own-Pytorch-3-Build-Classifier/)\n",
    "- [Pytorch: A Gentle Introduction to torch.autograd](https://docs.pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- [Automatic Differentiation with torch.autograd](https://docs.pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)\n",
    "\n",
    "### Secundárias\n",
    "\n",
    "- [Tom Roth: Building a computational graph: part 1](https://tomroth.dev/compgraph1/)\n",
    "- [Tom Roth: Building a computational graph: part 2](https://tomroth.dev/compgraph2/)\n",
    "- [Tom Roth: Building a computational graph: part 3](https://tomroth.dev/compgraph3/)\n",
    "- [Roger Grosse (Toronto) class on Automatic Differentiation](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n",
    "- [Computational graphs and gradient flows](https://simple-english-machine-learning.readthedocs.io/en/latest/neural-networks/computational-graphs.html)\n",
    "- [Colah Visual Blog: Backprop](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [Towards Data Science: Automatic Differentiation (AutoDiff): A Brief Intro with Examples](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b/)\n",
    "- [A Hands-on Introduction to Automatic Differentiation - Part 1](https://mostafa-samir.github.io/auto-diff-pt1/)\n",
    "- [Build Your own Deep Learning Framework - A Hands-on Introduction to Automatic Differentiation - Part 2](https://mostafa-samir.github.io/auto-diff-pt1/)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
